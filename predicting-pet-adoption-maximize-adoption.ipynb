{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8733542,"sourceType":"datasetVersion","datasetId":5242440}],"dockerImageVersionId":30732,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv(\"/kaggle/input/predict-pet-adoption-status-dataset/pet_adoption_data.csv\")\ndf_numeric = df\ndf_validation = df\ndf_rfe = df\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:37:45.540404Z","iopub.status.idle":"2024-06-21T07:37:45.540809Z","shell.execute_reply.started":"2024-06-21T07:37:45.540606Z","shell.execute_reply":"2024-06-21T07:37:45.540623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"adopt = df[df['AdoptionLikelihood']==1]\nno_adopt = df[df['AdoptionLikelihood']==0]\nprint(\"Number of Likely Adopted\",adopt[\"PetID\"].count())\nprint(\"Number of Unlikely Adopted\",adopt[\"PetID\"].count())","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:37:45.541983Z","iopub.status.idle":"2024-06-21T07:37:45.542507Z","shell.execute_reply.started":"2024-06-21T07:37:45.542240Z","shell.execute_reply":"2024-06-21T07:37:45.542263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**CORRELATION**\\\n\\\nSee the correlation of each feature against the target variable (Adoption Likelihood). The results do not provide much meaningful information because each feature has a low correlation with the target variable. Unless features *Vaccinated*, *HealthCondition*, *Breed*, and *AgeMonths* have a significant correlation with Adoption Likelihood rather than other features.","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\ndf['PetType'] = label_encoder.fit_transform(df['PetType'])\ndf['Breed'] = label_encoder.fit_transform(df['Breed'])\ndf['Color'] = label_encoder.fit_transform(df['Color'])\ndf['Size'] = label_encoder.fit_transform(df['Size'])\n\nbands = [\"PetType\",\"Breed\",\"AgeMonths\",\"Color\",\"Size\",\"WeightKg\",\"Vaccinated\",\"HealthCondition\",\"TimeInShelterDays\",\"AdoptionFee\",\"PreviousOwner\"]\nlabel = 'AdoptionLikelihood'\n\ncorrelation_coefficients = {band: stats.pointbiserialr(df[band], df[label])[0] for band in bands}\n\ncorrelation_df = pd.DataFrame(list(correlation_coefficients.items()), columns=['Band', 'Correlation'])\n\ncorrelation_df = correlation_df.sort_values(by='Correlation', ascending=False)\n\nsns.set(style=\"darkgrid\")\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Correlation', y='Band', data=correlation_df, palette='viridis')\nplt.title(\"Correlation Within Features and Adoption Likelihood\")\nplt.xlabel(\"\")\nplt.ylabel('')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**FEATURES SELECTION**","metadata":{}},{"cell_type":"markdown","source":"**Boxplot**\\\nUsing boxplot to compare distribution of numeric features for each label. The result shows that only *AgeMonths* feature has the significant difference on data distribution each label.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\n\ndf_numeric['Adoption Likelihood'] = df_numeric['AdoptionLikelihood'].replace({1: 'Yes', 0: 'No'})\n\nscaler = MinMaxScaler()\ndf_numeric[\"Age Months\"] = scaler.fit_transform(df_numeric[[\"AgeMonths\"]])\ndf_numeric[\"Weight Kg\"] = scaler.fit_transform(df_numeric[[\"WeightKg\"]])\ndf_numeric[\"Time in Shelter Days\"] = scaler.fit_transform(df_numeric[[\"TimeInShelterDays\"]])\ndf_numeric[\"Adoption Fee\"] = scaler.fit_transform(df_numeric[[\"AdoptionFee\"]])\n\nfeature_names = [\"Age Months\",\"Weight Kg\",\"Time in Shelter Days\",\"Adoption Fee\", \"Adoption Likelihood\"]\n\ndf_melted = pd.melt(df_numeric[feature_names], id_vars='Adoption Likelihood', var_name='Features', value_name='Value')\n\nplt.figure(figsize=(16, 8))\nsns.boxplot(x='Features', y='Value', hue='Adoption Likelihood', data=df_melted)\nplt.title('Boxplot Numeric Features')\nplt.xlabel('')\nplt.ylabel('')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ANOVA f-test**\n\\\nANOVA is an acronym for “analysis of variance” and is a parametric statistical hypothesis test for determining whether the means from two or more samples of data (often three or more) come from the same distribution or not.\n\\\n\\\nAn F-statistic, or F-test, is a class of statistical tests that calculate the ratio between variances values, such as the variance from two different samples or the explained and unexplained variance by a statistical test, like ANOVA. The ANOVA method is a type of F-statistic referred to here as an ANOVA f-test.\n\\\n\\\nImportantly, ANOVA is used when one variable is numeric and one is categorical, such as numerical input variables and a classification target variable in a classification task.\n\\\n\\\nThe results of this test can be used for feature selection where those features that are independent of the target variable can be removed from the dataset.\n\\\n\\\nsource: https://machinelearningmastery.com/feature-selection-with-numerical-input-data/\n\\\n\\\n**Result**\\\nThe results are the same as the results of the previous analysis, namely the *AgeMonths* feature has a significant effect on determining the target class.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\n\nX = df_numeric[[\"AgeMonths\",\"WeightKg\",\"TimeInShelterDays\",\"AdoptionFee\"]]\ny = df_numeric['AdoptionLikelihood']\n\ndef select_features(X_train, y_train, X_test):\n\tfs = SelectKBest(score_func=f_classif, k='all')\n\tfs.fit(X_train, y_train)\n\tX_train_fs = fs.transform(X_train)\n\tX_test_fs = fs.transform(X_test)\n\treturn X_train_fs, X_test_fs, fs\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\nX_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test)\n\nfor i in range(len(fs.scores_)):\n    print('Feature %s: %f' % (X.columns[i], fs.scores_[i]))\nplt.figure(figsize=(12, 6))\nplt.bar(X.columns, fs.scores_)\nplt.xticks(rotation=45)\nplt.xlabel('Features')\nplt.ylabel('ANOVA F-Value')\nplt.title('Feature Importance based on ANOVA F-test')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Chi-Square and P-Value**\n- High Chi-Square value: Indicates that there is a strong dependency between the feature and the target. The feature may be important to the model.\n- Low p-value (usually <0.05): Indicates that we can reject the null hypothesis. That is, there is a significant relationship between the feature and the target.\\\n\nSo the main objective is, find feature that has high chi-square value and low p-value.\\\nThe result is features *Breed*, *Vaccinated*, and *HealthCondition* are fulfill the requirements.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_selection import chi2\n\nX = df[[\"PetType\",\"Breed\",\"Color\",\"Size\",\"Vaccinated\",\"HealthCondition\",\"PreviousOwner\"]]\ny = df['AdoptionLikelihood']\nchi2_scores, p_values = chi2(X, y)\n\nchi2_results = pd.DataFrame({'Feature': [\"PetType\",\"Breed\",\"Color\",\"Size\",\"Vaccinated\",\"HealthCondition\",\"PreviousOwner\"],\n                             'Chi2_Score': chi2_scores,\n                             'P_Value': p_values})\nprint(chi2_results)\n\nsns.set(style=\"darkgrid\")\nplt.figure(figsize=(20, 10))  \nsns.lineplot(data=chi2_results, x=\"Feature\", y=\"Chi2_Score\", marker='o', label=\"Chi-Square\")\nsns.lineplot(data=chi2_results, x=\"Feature\", y=\"P_Value\", marker='o', label=\"P-Value\")\nplt.xlabel(\"Feature\")\nplt.ylabel(\"Value\")\nplt.title(\"Line Plot Chi-Square & P-Value of Predictor Attributes\")\nplt.legend()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Feature Importance Random Forest Classifier**\\\nThe goal is to find what features most influence the division of trees in the Random Forest Classifier algorithm.\\\nThe result is features *Size*, *AgeMonths*, *Breed*, and *Vaccinated*","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nX = df_rfe[[\"PetType\",\"Breed\",\"AgeMonths\",\"Color\",\"Size\",\"WeightKg\",\"Vaccinated\",\"HealthCondition\",\"TimeInShelterDays\",\"AdoptionFee\",\"PreviousOwner\"]]\ny = df_rfe['AdoptionLikelihood']\n\nrf_all_features = RandomForestClassifier()\nrf_all_features.fit(X, y)\n\nimportances = rf_all_features.feature_importances_\nfeature_importance_df = pd.DataFrame({'Feature': [\"PetType\",\"Breed\",\"AgeMonths\",\"Color\",\"Size\",\"WeightKg\",\"Vaccinated\",\"HealthCondition\",\"TimeInShelterDays\",\"AdoptionFee\",\"PreviousOwner\"], 'Importance': importances})\nfeature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Importance', y='Feature', data=feature_importance_df)\nplt.title('Feature Importance with All Features')\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So the features selected in the feature selection process are features *Size*, *AgeMonths*, *Breed*, *Vaccinated*, and *HealthCondition*","metadata":{}},{"cell_type":"markdown","source":"**Measure The Influence of Each Feature on The Accuracy Value of The Model**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.inspection import permutation_importance\n\nX = df[[\"Breed\", \"AgeMonths\", \"Vaccinated\", \"HealthCondition\", \"Size\"]]\ny = df['AdoptionLikelihood']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodels = {\n    'Logistic Regression': LogisticRegression(),\n    'Decision Tree': DecisionTreeClassifier(),\n    'Random Forest': RandomForestClassifier(),\n    'Support Vector Machine': SVC(),\n    'K-Nearest Neighbors': KNeighborsClassifier(),\n    'Naive Bayes': GaussianNB(),\n    'Gradient Boosting': GradientBoostingClassifier(),\n    'AdaBoost': AdaBoostClassifier(),\n    'Neural Network': MLPClassifier(max_iter=1000),\n    'Linear Discriminant Analysis': LinearDiscriminantAnalysis()\n}\n\naccuracy_results = []\nfeature_importance_values = []\n\nfor model_name, model in models.items():\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    accuracy_results.append((model_name, accuracy))\n    \n    result = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)\n    feature_importance_values.append(result.importances_mean)\n\naccuracy_df = pd.DataFrame(accuracy_results, columns=['Model', 'Accuracy'])\naccuracy_df = accuracy_df.sort_values(by='Accuracy', ascending=False).reset_index(drop=True)\n\naverage_importance = np.mean(feature_importance_values, axis=0)\n\nfeature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': average_importance})\nfeature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n\nplt.figure(figsize=(14, 7))\nsns.barplot(x='Importance', y='Feature', data=feature_importance_df)\nplt.title('Average Feature Importance (Sorted)')\nplt.xlabel('Importance')\nplt.ylabel('Features')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Find The Best Model for Classification**","metadata":{}},{"cell_type":"code","source":"X = df[[\"Breed\",\"AgeMonths\",\"Vaccinated\",\"HealthCondition\",\"Size\"]]\ny = df['AdoptionLikelihood']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodels = {\n    'Logistic Regression': LogisticRegression(),\n    'Decision Tree': DecisionTreeClassifier(),\n    'Random Forest': RandomForestClassifier(),\n    'Support Vector Machine': SVC(),\n    'K-Nearest Neighbors': KNeighborsClassifier(),\n    'Naive Bayes': GaussianNB(),\n    'Gradient Boosting': GradientBoostingClassifier(),\n    'AdaBoost': AdaBoostClassifier(),\n    'Neural Network': MLPClassifier(max_iter=1000),\n    'Linear Discriminant Analysis': LinearDiscriminantAnalysis()\n}\n\naccuracy_results = []\n\nfor model_name, model in models.items():\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    accuracy_results.append((model_name, accuracy))\n\naccuracy_df = pd.DataFrame(accuracy_results, columns=['Model', 'Accuracy'])\naccuracy_df = accuracy_df.sort_values(by='Accuracy', ascending=False).reset_index(drop=True)\n\nplt.figure(figsize=(14, 7))\nsns.lineplot(data=accuracy_df, x='Model', y='Accuracy', marker='o')\nfor i, row in accuracy_df.iterrows():\n    plt.text(i, row['Accuracy'], f\"{row['Accuracy']*100:.2f}%\", horizontalalignment='center', size='medium', color='black', weight='semibold')\nplt.xlabel('Model')\nplt.ylabel('Accuracy')\nplt.title('Accuracy of Different Classification Models (Feature Selection)')\nplt.xticks(rotation=45)\nplt.ylim(0, 1)\nplt.grid(True)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Hyperparameter Tuning Ada Boost Classifier**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom scipy.stats import randint\n\nparam_dist = {\n    'n_estimators': randint(50, 500),\n    'learning_rate': np.linspace(0.01, 1, num=50),\n    'estimator__max_depth': randint(1, 10),\n    'estimator__min_samples_split': randint(2, 20),\n    'estimator__min_samples_leaf': randint(1, 20)\n}\n\nada = AdaBoostClassifier(estimator=DecisionTreeClassifier())\nrandom_search = RandomizedSearchCV(estimator=ada, param_distributions=param_dist, n_iter=100, cv=3, verbose=2, random_state=1, n_jobs=-1, scoring='accuracy')\nrandom_search.fit(X_train, y_train)\n\nprint(f\"Best parameters found: {random_search.best_params_}\")\nprint(f\"Best cross-validation accuracy: {random_search.best_score_}\")\n\nbest_ada = random_search.best_estimator_\ny_pred = best_ada.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Test accuracy: {accuracy}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Classification Report Ada Boost Classifier**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nbest_ada.fit(X_train, y_train)\nada_pred = best_ada.predict(X_test)\nprint(\"Classification report\")\nprint(classification_report(y_test,ada_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Confusion Matrix Ada Boost Classifier**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\ndf['Label'] = df['AdoptionLikelihood'].replace({1: 'Yes', 0: 'No'})\n\naux_df = df[['Label', 'AdoptionLikelihood']].drop_duplicates().sort_values('AdoptionLikelihood')\nconf_matrix = confusion_matrix(y_test, ada_pred)\n\nplt.figure(figsize=(10,8))\nsns.heatmap(conf_matrix, \n            annot=True,\n            xticklabels=aux_df['Label'].values, \n            yticklabels=aux_df['Label'].values,\n            cmap=\"Blues\")\nplt.ylabel('Predicted')\nplt.xlabel('Actual')\nplt.title('Confusion matrix')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Try to Use All Features**\\\n2 best model is **Random Forest Classifier** and **Gradient Boost Classifier**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.metrics import accuracy_score\n\nX = df[[\"PetType\",\"Breed\",\"AgeMonths\",\"Color\",\"Size\",\"WeightKg\",\"Vaccinated\",\"HealthCondition\",\"TimeInShelterDays\",\"AdoptionFee\",\"PreviousOwner\"]]\ny = df['AdoptionLikelihood']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodels = {\n    'Logistic Regression': LogisticRegression(),\n    'Decision Tree': DecisionTreeClassifier(),\n    'Random Forest': RandomForestClassifier(),\n    'Support Vector Machine': SVC(),\n    'K-Nearest Neighbors': KNeighborsClassifier(),\n    'Naive Bayes': GaussianNB(),\n    'Gradient Boosting': GradientBoostingClassifier(),\n    'AdaBoost': AdaBoostClassifier(),\n    'Neural Network': MLPClassifier(max_iter=1000),\n    'Linear Discriminant Analysis': LinearDiscriminantAnalysis()\n}\n\naccuracy_results = []\n\nfor model_name, model in models.items():\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    accuracy_results.append((model_name, accuracy))\n\naccuracy_df = pd.DataFrame(accuracy_results, columns=['Model', 'Accuracy'])\naccuracy_df = accuracy_df.sort_values(by='Accuracy', ascending=False).reset_index(drop=True)\n\nplt.figure(figsize=(14, 7))\nsns.lineplot(data=accuracy_df, x='Model', y='Accuracy', marker='o')\nfor i, row in accuracy_df.iterrows():\n    plt.text(i, row['Accuracy'], f\"{row['Accuracy']*100:.2f}%\", horizontalalignment='center', size='medium', color='black', weight='semibold')\nplt.xlabel('Model')\nplt.ylabel('Accuracy')\nplt.title('Accuracy of Different Classification Models (Without Feature Selection)')\nplt.xticks(rotation=45)\nplt.ylim(0, 1)\nplt.grid(True)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Hyperparameter Tuning Random Forest Classifier**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom scipy.stats import randint\n\nparam_dist = {\n    'n_estimators': randint(100, 1000),\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth': [None] + list(np.arange(10, 110, 10)),\n    'min_samples_split': randint(2, 20),\n    'min_samples_leaf': randint(1, 20),\n    'bootstrap': [True, False]\n}\n\nrf = RandomForestClassifier()\nrandom_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, n_iter=100, cv=3, verbose=2, random_state=1, n_jobs=-1, scoring='accuracy')\nrandom_search.fit(X_train, y_train)\n\nprint(f\"Best parameters found: {random_search.best_params_}\")\nprint(f\"Best cross-validation accuracy: {random_search.best_score_}\")\n\nbest_rf = random_search.best_estimator_\ny_pred = best_rf.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Test accuracy: {accuracy}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Classification Report Random Forest Classifier**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\n\nbest_rf.fit(X_train, y_train)\nrf_pred = best_rf.predict(X_test)\nprint(\"Classification report\")\nprint(classification_report(y_test,rf_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Confusion Matrix Random Forest Classifier**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf['Label'] = df['AdoptionLikelihood'].replace({1: 'Yes', 0: 'No'})\n\naux_df = df[['Label', 'AdoptionLikelihood']].drop_duplicates().sort_values('AdoptionLikelihood')\nconf_matrix = confusion_matrix(y_test, rf_pred)\n\nplt.figure(figsize=(10,8))\nsns.heatmap(conf_matrix, \n            annot=True,\n            xticklabels=aux_df['Label'].values, \n            yticklabels=aux_df['Label'].values,\n            cmap=\"Blues\")\nplt.ylabel('Predicted')\nplt.xlabel('Actual')\nplt.title('Confusion matrix')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Hyperparameter Tuning Gradient Boost Classifier**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\nfrom scipy.stats import uniform, randint\n\nparam_distributions = {\n    'n_estimators': randint(100, 1000),\n    'learning_rate': uniform(0.01, 0.3),\n    'max_depth': randint(3, 10),\n    'min_samples_split': randint(2, 20),\n    'min_samples_leaf': randint(1, 20),\n    'subsample': uniform(0.7, 0.3),\n    'max_features': ['auto', 'sqrt', 'log2', None]\n}\n\ngb = GradientBoostingClassifier()\nrandom_search = RandomizedSearchCV(estimator=gb, param_distributions=param_distributions, n_iter=100, cv=3, n_jobs=-1, verbose=2, random_state=1, scoring='accuracy')\nrandom_search.fit(X_train, y_train)\n\nprint(f\"Best parameters found: {random_search.best_params_}\")\nprint(f\"Best cross-validation accuracy: {random_search.best_score_}\")\n\nbest_gb = random_search.best_estimator_\ny_pred = best_gb.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Test accuracy: {accuracy}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Classification Report Gradient Boost Classifier**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\n\nbest_gb.fit(X_train, y_train)\ngb_pred = best_gb.predict(X_test)\nprint(\"Classification report\")\nprint(classification_report(y_test,gb_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Confusion Matrix Gradient Boost Classifier**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf['Label'] = df['AdoptionLikelihood'].replace({1: 'Yes', 0: 'No'})\n\naux_df = df[['Label', 'AdoptionLikelihood']].drop_duplicates().sort_values('AdoptionLikelihood')\nconf_matrix = confusion_matrix(y_test, gb_pred)\n\nplt.figure(figsize=(10,8))\nsns.heatmap(conf_matrix, \n            annot=True,\n            xticklabels=aux_df['Label'].values, \n            yticklabels=aux_df['Label'].values,\n            cmap=\"Blues\")\nplt.ylabel('Predicted')\nplt.xlabel('Actual')\nplt.title('Confusion matrix')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**VALIDATION**\\\nFind the value of each important feature that most influences a pet's adoption rate.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nadopt = df_validation[df_validation['AdoptionLikelihood']==1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Medium-Sized Pets**","metadata":{}},{"cell_type":"code","source":"categories = adopt[\"Size\"].unique()\nvalues = adopt[\"Size\"].value_counts()\n\nplt.figure(figsize=(10, 3))  \nplt.bar(categories, values, color='skyblue') \nplt.title('Pets Size are Likely to be Adopted')\nplt.xlabel('Size')\nplt.ylabel('Count')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Vaccinated Pets**","metadata":{}},{"cell_type":"code","source":"adopt['Vaccinated'] = adopt['Vaccinated'].replace({0: 'Vaccinated', 1: 'Not Vaccinated'})\n\ncategories = adopt[\"Vaccinated\"].unique()\nvalues = adopt[\"Vaccinated\"].value_counts()\n\nplt.figure(figsize=(10, 3))  \nplt.bar(categories, values, color='skyblue') \nplt.title('Pets Vaccination Status that are Likely to be Adopted')\nplt.xlabel('Breed')\nplt.ylabel('Count')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Healthy Pets**","metadata":{}},{"cell_type":"code","source":"adopt['HealthCondition'] = adopt['HealthCondition'].replace({0: 'Healthy', 1: 'Medical Condition'})\n\ncategories = adopt[\"HealthCondition\"].unique()\nvalues = adopt[\"HealthCondition\"].value_counts()\n\nplt.figure(figsize=(10, 3))  \nplt.bar(categories, values, color='skyblue') \nplt.title('Pets Health Condtition that are Likely to be Adopted')\nplt.xlabel('Breed')\nplt.ylabel('Count')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Puppy, Kitten, Litle Bird, Bunny**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 3))\nplt.hist(adopt[\"AgeMonths\"], bins=30, color='skyblue', edgecolor=\"black\")\nplt.title('Pets Age Months are Likely to be Adopted')\nplt.ylabel('Count')\nplt.xlabel('Age Months')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Labrador Dog**","metadata":{}},{"cell_type":"code","source":"categories = adopt[\"Breed\"].unique()\nvalues = adopt[\"Breed\"].value_counts()\n\nplt.figure(figsize=(10, 3))  \nplt.bar(categories, values, color='skyblue') \nplt.title('Breeds that are Likely to be Adopted')\nplt.xlabel('Breed')\nplt.ylabel('Count')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**CONCLUSION**\n- The features that most influence the adoption rate are (most-least) **(Size - Vaccination - Health Condition - Age Months - Breed)**\n- So the strategies to increase adoption rates in shelters:\n1. The pets that have a medium size are more likely to be adopted. So increase the stock of medium-sized pets\n2. The pets that already vaccinated are more likely to be adopted. So it is necessary to vaccinate all pets first before they are ready for adoption.\n3. The pets that are healthy are more likely to be adopted. So it is necessary to ensure that the pet to be adopted is in good health.\n4. The pets that have a small number of age months are more likely to be adopted. So provide more pets such as puppies, kittens, bunny, and others.\n5. The labrador breed is the most likely pet to be adopted\n- Using features from the feature selection result; between Ada Boost Classifier, Random Forest Classifier, and Gradient Boost Classifier the best model is **Ada Boost Classifier** with accuracy around **96%**:   \n\n                 precision   recall  f1-score   support\n\n           0        0.97      0.98      0.97       259\n           1        0.96      0.94      0.95       143\n\n      accuracy                          0.96       402\n      macro avg     0.96      0.96      0.96       402\n      weighted avg  0.96      0.96      0.96       402","metadata":{}}]}